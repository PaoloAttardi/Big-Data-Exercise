{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Template prove vecchie.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"zCyUPNEfbuRj"},"source":["# The STEPS FOR DATA ANALISYS AND ML PROBLEMS :\n","- 1 - load and plot (optional) data to understand the problem\n","- 2 - (optional, recommended) data normalization / standardization - great influence on models that estimate on distances - gradient descent much faster\n","- 3 - (optional) feature selection / dimensionality reduction\n","- 4 - algorithm comparison and choice - train with cross validation ----- NB : scoring!!\n","- 5 - validation curve / learning curve\n","- 6 - (optional) algorithm tuning\n","- 7 - make predictions - confusion matrix & classification report\n","\n","\n","NB : always feature selection AFTER feature scaling\n","\n","ps -> no validation curve or learning curve in this file. Search in the other if needed"]},{"cell_type":"code","metadata":{"id":"wHUUEOg6buRx"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","\n","from sklearn.preprocessing import Normalizer\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import SelectKBest, RFE\n","\n","#classification\n","from sklearn.svm import SVC\n","from sklearn.svm import LinearSVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","\n","#regression\n","from sklearn.svm import LinearSVR\n","from sklearn.svm import SVR\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge #least squares\n","from sklearn.linear_model import Lasso\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import cross_val_score\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lWTOrQ6kbuR0"},"source":["data = pd.read_csv('weather.csv', sep=';')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WXJSo7SlbuR3"},"source":["DATA VISUALIZATION"]},{"cell_type":"code","metadata":{"id":"TKtGcfgqbuR4"},"source":["data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lw0LtY89buR5"},"source":["data.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ci57M7fpbuR9"},"source":["data.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2vKcq87buR_"},"source":["data.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CceV4Cb9buSA"},"source":["# check for nan values\n","data[data.isna().any(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ML4xgWOmbuSB"},"source":["# check for all zeros columns\n","data.columns[(data == 0).all()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6s7BrVcbuSB"},"source":["# dataset bilanciato 50:50\n","data.groupby('y').size().plot(kind='bar')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjEPEOySbuSF"},"source":["# visualize one column distribution\n","data.groupby('age').size().plot()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FhY7xIi6buSG"},"source":["# counting rows for each unique value (feature)\n","data.groupby('Location').size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2A9dQV7TbuSG"},"source":["# plotting with multi indexing - groupby su più chiavi\n","\n","toplot = data.groupby(['marital', 'y']).size().unstack()\n","toplot.plot()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBL7wqaVbuSH"},"source":["# NBB\n","# plotting tha same things as before, but a graph for each first key\n","toplot = data.groupby(['marital', 'y']).size()\n","\n","for i in toplot.index.levels[0]:\n","    toplot.loc[i].plot(kind='bar', title=i)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDz4jIYvbuSI"},"source":["# per capire se una classe influisce su quella di predizione ---- y è quella di predizione\n","toplot = dataset.groupby(['marital', 'y']).size()\n","total = dataset.groupby('y').size()\n","# automaticamente posso dividere per ogni indice\n","(toplot / total).unstack().plot(kind='bar')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2w3bq9KbuSI"},"source":["# plottare la media di una coppia di feature\n","toplot = dataset.groupby(['sex','G3']).size()# do .unstack() after, so we can divide by index\n","total = dataset.groupby('sex').size()\n","\n","(toplot / total).unstack().plot(kind='bar')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAZxzRufbuSJ"},"source":["# plot multiple curve on same graph\n","padri.plot(label='padri')\n","madri.plot(label='madri')\n","plt.xlabel('education')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugcSISdpbuSJ"},"source":["# per identificare una picco in un istogramma su una feature\n","data['satisfaction_level'].hist(bins=100) # guardo istogramma per vedere numero di istanze\n","data.groupby('satisfaction_level').size() # associo valore ad istanze\n","data[data['satisfaction_level'] <= 0.11] # seleziono istanze\n","# confronto con dati normali con describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItUFQrYzbuSJ"},"source":["# media e mediana di una feature\n","data['satisfaction_level'].describe()\n","\n","# usare describe per confrontare due dataset / colonne"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27liR2MLbuSJ"},"source":["hum = data[['Month', 'Location', 'Humidity9am', 'Humidity3pm']]\n","# per ogni coppia citta mese calcolo le temperature minime e massime calcolate al mattino e al pome\n","hum_max = hum.groupby(['Location', 'Month']).max()\n","hum_min = hum.groupby(['Location', 'Month']).min()\n","# qui cerco la massima/minima tra le due ----> mi fornisce quella giornaliera\n","hum_max = hum_max.max(axis=1)\n","hum_min = hum_min.min(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3PrpYY4cbuSK"},"source":["DATA TRANSFORMATION"]},{"cell_type":"code","metadata":{"id":"J6R0dOknbuSM"},"source":["# how to create a copy of a dataset\n","datanew = dataset.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfvWp73ObuSN"},"source":["# drop rows with nan instances\n","data = data.dropna() # axis=1 drop columns\n","# fillna valuese\n","data = data.fillna(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7UpMEplbuSO"},"source":["# drop columns\n","data = data.drop([coltoremove], axis=1)\n","# drop rows\n","dataset_n = dataset_n.drop(toremove.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIAjmsC2buSQ"},"source":["#come si aggiungono colonne in modo idiomatico --- NB - viene messa alla fine\n","\n","# new attributo escursione termica\n","data['escursione'] = data['MaxTemp'] - data['MinTemp']\n","\n","# in alternativa va bene anche \n","tmp = pd.DataFrame(data['MaxTemp'] - data['MinTemp'], columns=['escursione'])\n","\n","#metto la colonna all'inizio -- si potrebbe anche concatenare (data[:, -1:],data[:, :-1])\n","data = tmp.join(data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHmO8c6RbuSS"},"source":["# rimpiazzare valori nominali con numeri\n","data = data.replace(['Yes', 'No'],[1,0])\n","# same ----> data.replace({'yes':1, 'no':0})\n","\n","#rimpiazzo solo una colonna\n","cities = data['Location'].unique()\n","data['Location'] = data['Location'].replace(cities, np.arange(len(cities)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Yx7i-OmbuSV"},"source":["# scegliere solo un subset di colonne da rimpiazzare\n","data.columns[data.dtypes == 'object']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfZQFjXAbuSX"},"source":["# rimpiazzo tutte le colonne con valori numerici\n","for col in tomap.columns:\n","    uniquev = tomap[col].unique()\n","    tomap.loc[:, col] = tomap.loc[:, col].replace(uniquev, np.arange(len(uniquev))) # loc non è necessario ma non da warning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oh1TgA8fbuSY"},"source":["# always check values after\n","data.dtypes\n","# if some values still not int\n","data['Location'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tuFzZeTNbuSZ"},"source":["# dummies features su TUTTO il dataset ------ NB : ignora i valori float!!!!!! BUONO!\n","# NON IMPORTA SE I VALORI SONO STRINGHE ------ NON SERVE CONVERTIRE LE FEATURE IN NUMERI\n","data_dummy = pd.get_dummies(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owybI_b0buSZ"},"source":["# creare un nuovo dataframe con dummies features solo su determinate colonne\n","\n","new_data = data_numeric.copy()\n","\n","todummy = 'age_new' # colonne da dummizzare could be data.dtypes == 'object'\n","dummies = pd.get_dummies(data_numeric[todummy])\n","\n","new_data = new_data.drop(todummy, axis=1)\n","new_data = new_data.join(dummies)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2AX6EsebuSa"},"source":["# creare nuovi dataset\n","# new datasets from orginal dataset\n","cloudP = datini[datini['Cloud3pm'] < 0]\n","cloudT = datini[-(datini['Cloud3pm'] < 0)] # the sign - revert a boolean series"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6KJDkMjbuSa"},"source":["# ricevere dati numeri da dataframe\n","data_numeric = data._get_numeric_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5cfDEdPGbuSc"},"source":["# database basato solo su alcune colonne\n","data_color = data.loc[:, [0,3,9,14,15,17,20]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPMWVvBibuSe"},"source":["# eventuale renaming delle colonne\n","data_color = data_color.rename(columns={0:'p',\n","                                        3:'c1',\n","                                        9:'c2',\n","                                        14:'c3',\n","                                        15:'c4',\n","                                        17:'c5',\n","                                        20:'c6'})\n","\n","# se voglio rinominare tutte le colonne in con una lista\n","data_color.columns = ['p', 'c1', 'c2',...]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rOLqs1BAbuSf"},"source":["DATASET BALANCE"]},{"cell_type":"code","metadata":{"id":"7j6I6myZbuSg"},"source":["# balance a data with resampling\n","# bigger rules or intermidiate value\n","from sklearn.utils import resample\n","\n","bigger = data_air[data_air['SIZE']==1] \n","smaller = data_air[data_air['SIZE']==3]\n","\n","# bilanciamo le classi \n","smaller = resample(smaller, replace=True, n_samples=5000) # can be also n_samples=len(bigger)\n","bigger = resample(bigger, replace=True, n_samples=5000) # non cessary if previous n_samples = len(bigger)\n","\n","data_bal = pd.concat([bigger, smaller])\n","data_bal.groupby('SIZE').size()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPZGcmyqbuSh"},"source":["DATA PREPARATION"]},{"cell_type":"markdown","metadata":{"id":"Fa-RlMfObuSi"},"source":["NB posso spezzare i dati in mquesto modo se non mi è stata fornita nessuna specifica"]},{"cell_type":"code","metadata":{"id":"XH54ftpfbuSi"},"source":["# nel caso la colonna di classificazione non sia in fondo possiamo usare questa notazione\n","X = data.drop('Cloud3pm', axis=1)\n","y = data['Cloud3pm']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_uZbqrsbuSi"},"source":["# dividere il dataset in un N intervalli \n","# ---> ritorna array contenente indice di appartenenza ad un bin\n","interval = pd.cut(data_numeric['age'], bins=3, labels=False) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YM6T8TjXbuSj"},"source":["# nel caso dobbiamo dividere per ogni classe di size\n","# NBBBB-----> automaticamente con train_test_split() prendendo i valori a random\n","# ----------> le proprietà dovrebbero rimane quasi rispettate (ma non sarà perfetto)\n","\n","index1 = data_bal['SIZE'] == 1\n","index3 = data_bal['SIZE'] == 3\n","index5 = data_bal['SIZE'] == 5\n","\n","X_1 = data_bal[index1].drop('SIZE', axis=1)\n","y_1 = data_bal.loc[index1, 'SIZE']\n","\n","X_3 = data_bal[index3].drop('SIZE', axis=1)\n","y_3 = data_bal.loc[index3, 'SIZE']\n","\n","X_5 = data_bal[index5].drop('SIZE', axis=1)\n","y_5 = data_bal.loc[index5, 'SIZE']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TaYdlc_xbuSk"},"source":["# andiamo poi ad unire le classi\n","fraction = 0.25\n","\n","X_1train, X_1test, y_1train, y_1test = train_test_split(X_1, y_1, test_size=fraction)\n","X_3train, X_3test, y_3train, y_3test = train_test_split(X_3, y_3, test_size=fraction)\n","X_5train, X_5test, y_5train, y_5test = train_test_split(X_5, y_5, test_size=fraction)\n","\n","X_train = pd.concat([X_5train, X_3train, X_1train])\n","X_test = pd.concat([X_5test, X_3test, X_1test])\n","\n","y_train = pd.concat([y_5train, y_3train, y_1train])\n","y_test = pd.concat([y_5test, y_3test, y_1test])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hl9rXn-BbuSl"},"source":["fraction = 0.2\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=fraction)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUulQuNJbuSl"},"source":["# convert y to ndarray -----> solo se fatto senza train_test_split()\n","\n","y_train = np.ravel(y_train)\n","y = np.ravel(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAe2sAr9buSm"},"source":["Non vado a normalizzare le componenti che sono attributi di tipo nominale, NON AVREBBE SENSO"]},{"cell_type":"code","metadata":{"id":"s-MLIDqqbuSm"},"source":["norm = Normalizer()\n","#X_new_train = norm.fit_transform(X_train)\n","#X_new_test = norm.fit_transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ki63XgScbuSn"},"source":["# ora procedo al dimensionality reduction con pca\n","pca = PCA(n_components=10)\n","#X_new_train = pca.fit_transform(X_new_train)\n","#X_new_test = pca.fit_transform(X_new_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pC7ihtFMbuSo"},"source":["# oppure con univariate\n","uni_sel = SelectKBest(k=10)\n","#X_new = uni_sel.fit_transform(X_new, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o4FZqUQIbuSq"},"source":["# oppure con recursive feature elimination\n","#rfe = RFE()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhn3S_pSbuSq"},"source":["# distribuzione delle feature\n","tmp = pd.DataFrame(X_train)\n","tmp.hist(figsize=(12,5), bins=50)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zxTA__ehbuSq"},"source":["# scatter matrix\n","# può essere molto lento, da evitare con grandi dataset e molte features\n","# pd.plotting.scatter_matrix(tmp)\n","# plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6SlL65MbuSq"},"source":["# correlation matrix\n","plt.matshow(tmp.corr())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_VAVw69ZbuSr"},"source":["implement a model (eg. logistic regression)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"4wJYu0dYbuSt"},"source":["# testare accuracy su modello\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","\n","model = LogisticRegression(solver='lbfgs', max_iter=1000) # parameter change for each algorithms look down\n","model.fit(X_train, y_train)\n","\n","predicted = model.predict(X_test)\n","\n","print('accuracy :')\n","print(accuracy_score(y_test, predicted))\n","print('confusion matrix :')\n","print(confusion_matrix(y_test, predicted))\n","print('classification report :')\n","print(classification_report(y_test, predicted))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1BU1VmZDbuSu"},"source":["Running cross validation test on algoritmhs - accuracy/f1 score"]},{"cell_type":"code","metadata":{"id":"pSkocffzbuSw"},"source":["# cross validation on single model\n","model = DecisionTreeClassifier()\n","num_folds = 5\n","scoring = 'accuracy'\n","\n","results = cross_val_score(model, X, y, scoring=scoring, cv=num_folds)\n","\n","results.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxG9Fce8buSy"},"source":["# algorithm choice\n","\n","models = []\n","\n","\n","#CLASSIFICATION\n","\n","#models.append(('lSVM', LinearSVC() ))\n","#models.append(('SVM', SVC(gamma='scale') )) # can be also 'auto'\n","#models.append(('p2SVM', SVC(gamma='scale', kernel='poly', degree=2) ))\n","#models.append(('p3SVM', SVC(gamma='scale', kernel='poly', degree=3) ))\n","#models.append(('p4SVM', SVC(gamma='scale', kernel='poly', degree=4) ))\n","#models.append(('AB', AdaBoostClassifier() ))\n","#models.append(('NB', GaussianNB() ))\n","#models.append(('MNB', MultinomialNB() ))\n","models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=1500) ))\n","models.append(('DT', DecisionTreeClassifier() ))\n","#models.append(('RF', RandomForestClassifier(n_estimators=10) ))\n","#models.append(('NN', MLPClassifier(max_iter=5000) )) troppo lento\n","#models.append(('KNN', KNeighborsClassifier() ))\n","\n","# REGRESSION\n","#models.append(('lSVR', LinearSVR() ))\n","#models.append(('SVR', SVR(gamma='scale') ))\n","#models.append(('p2SVM', SVR(gamma='scale', kernel='poly', degree=2) ))\n","#models.append(('p3SVM', SVR(gamma='scale', kernel='poly', degree=3) ))\n","#models.append(('p4SVM', SVR(gamma='scale', kernel='poly', degree=4) ))\n","#models.append(('AB', AdaBoostRegressor() ))\n","#models.append(('LR', LinearRegression() ))\n","#models.append(('DT', DecisionTreeRegressor() ))\n","#models.append(('RI', Ridge() ))\n","#models.append(('LA', Lasso() ))\n","#models.append(('RF', RandomForestRegressor(n_estimators=10) ))\n","#models.append(('KNN', KNeighborsRegressor() ))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vbiF7YebuSy"},"source":["# personalized scorer - eg. measuring accuracy of rounded regression results WITH CROSS VALIDATION\n","from sklearn.metrics import make_scorer\n","\n","def accuracy_reg(y, y_pred, **kwargs):\n","    y_pred = np.round(y_pred)\n","    errors = np.sum(np.abs(y - y_pred))\n","    correct = len(y) - errors\n","    return correct/len(y)\n","\n","# personal scorer instance\n","accuracy_r = make_scorer(accuracy_reg)\n","scoring = accuracy_r"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rSbFtqJMbuSy"},"source":["# scoring\n","# classification\n","scoring = 'accuracy' # 'f1_micro'\n","# regression\n","#scoring ='neg_mean_squared_error' # 'neg_mean_absolute_error' \n","\n","# testing models\n","results = []\n","names = []\n","num_folds = 5\n","\n","for name, model in models:\n","    \n","    t = time.process_time()\n","    # be careful, use (X, y) or (X_train, y_train) depending on how data were splitted\n","    cv_result = cross_val_score(model, X_train, y_train, scoring=scoring, cv=num_folds)\n","    t = time.process_time() - t\n","    results.append(cv_result)\n","    names.append(name)\n","    \n","    print('time required by', name,' : ', t)\n","    \n","    #the confusione matrix è la matrice dei true positive ecc...\n","    \n","    #Xtrain, Xtest, ytrain, ytest = train_test_split(X_new, y)\n","    #model.fit(Xtrain, ytrain)\n","    #predicted = model.predict(Xtest)\n","    #print(confusion_matrix(ytest, predicted))\n","print('results for ', scoring, ':')\n","frame = pd.DataFrame(results, index=names)\n","frame.mean(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94YwMevEbuSz"},"source":["fig = plt.figure()\n","fig.suptitle('Algorithm comparison accuracy')\n","ax = fig.add_subplot(111)\n","plt.boxplot(frame)\n","ax.set_xticklabels(frame.index)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZH7l4AxTbuSz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kdK14SG_buS0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLz5dMGCbuS0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wcVJehvGbuS0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ye3yXV_2buS0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0O8jzDEhbuS1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ihA51JdebuS1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8pVGGICNbuS1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9G3qTy8CbuS1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kM0BwKEVbuS2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wM1UVgKabuS2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4U3ab6SbuS2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3zfGBjxBbuS2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrngILmZbuS4"},"source":[""],"execution_count":null,"outputs":[]}]}